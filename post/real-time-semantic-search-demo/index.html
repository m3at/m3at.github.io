<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Real-time semantic search demo - Paul's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="Paul Willot"><meta name=description content="TLDR: I wanted to make a fast and low complexity &amp;ldquo;search engine&amp;rdquo;, and got good results with neural network embeddings combined with an approximate vector search.
Recent advances in large deep learning models have brought interesting multi-modal capabilities, notably for combined text and images, like Imagen and DALL-E 2 for image synthesis from raw text. CLIP, which is also used in DALL-E for image ranking, allow projecting both text and images into a coherent space."><meta name=keywords content="ML,CS"><meta name=generator content="Hugo 0.79.1 with theme even"><link rel=canonical href=/post/real-time-semantic-search-demo/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.d70ccb6824a5ce6b30b3758201981307a7fcc24b19562e15fbbdf20fcdf12f61.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Real-time semantic search demo"><meta property="og:description" content="TLDR: I wanted to make a fast and low complexity &ldquo;search engine&rdquo;, and got good results with neural network embeddings combined with an approximate vector search.
Recent advances in large deep learning models have brought interesting multi-modal capabilities, notably for combined text and images, like Imagen and DALL-E 2 for image synthesis from raw text. CLIP, which is also used in DALL-E for image ranking, allow projecting both text and images into a coherent space."><meta property="og:type" content="article"><meta property="og:url" content="/post/real-time-semantic-search-demo/"><meta property="article:published_time" content="2022-06-15T04:50:04+00:00"><meta property="article:modified_time" content="2022-06-15T04:50:04+00:00"><meta itemprop=name content="Real-time semantic search demo"><meta itemprop=description content="TLDR: I wanted to make a fast and low complexity &ldquo;search engine&rdquo;, and got good results with neural network embeddings combined with an approximate vector search.
Recent advances in large deep learning models have brought interesting multi-modal capabilities, notably for combined text and images, like Imagen and DALL-E 2 for image synthesis from raw text. CLIP, which is also used in DALL-E for image ranking, allow projecting both text and images into a coherent space."><meta itemprop=datePublished content="2022-06-15T04:50:04+00:00"><meta itemprop=dateModified content="2022-06-15T04:50:04+00:00"><meta itemprop=wordCount content="472"><meta itemprop=keywords content="machine-learning,search,web,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Real-time semantic search demo"><meta name=twitter:description content="TLDR: I wanted to make a fast and low complexity &ldquo;search engine&rdquo;, and got good results with neural network embeddings combined with an approximate vector search.
Recent advances in large deep learning models have brought interesting multi-modal capabilities, notably for combined text and images, like Imagen and DALL-E 2 for image synthesis from raw text. CLIP, which is also used in DALL-E for image ranking, allow projecting both text and images into a coherent space."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>paulw.tokyo</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>paulw.tokyo</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Real-time semantic search demo</h1><div class=post-meta><span class=post-time>2022-06-15</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents></nav></div></div><div class=post-content><p><strong><em>TLDR</em></strong><em>: I wanted to make a fast and low complexity &ldquo;search engine&rdquo;, and got good results with neural network embeddings combined with an approximate vector search.</em></p><p>Recent advances in large deep learning models have brought interesting multi-modal capabilities, notably for combined text and images, like <a href=https://imagen.research.google/>Imagen</a> and <a href=https://openai.com/dall-e-2/>DALL-E 2</a> for image synthesis from raw text. <a href=https://openai.com/blog/clip/>CLIP</a>, which is also used in DALL-E for image ranking, allow projecting both text and images into a coherent space.</p><p>I want to be able to search images from text, so using CLIP we can project both into the same space, making them comparable. Concretely we obtain vectors, and for pairs of vectors we can take their norm as a distance to find closest ones, and return that as a search result.</p><p>In practice comparing one vector (example our query) to all existing ones could be very slow if done exactly. Luckily <em>approximate</em> nearest neighbor (ANN) methods allow us to trade-off a tiny bit of accuracy for massive speed-ups and memory savings. Lots of methods exist (<a href=http://ann-benchmarks.com/>here for benchmarks</a> and <a href=https://www.pinecone.io/learn/>pinecone</a> has great docs as an introduction), including innovative <a href=https://arxiv.org/abs/2110.05789>learned quantization methods</a> that achieve excellent compression.</p><p>For the user interaction, any web tech stack can do the job, I went with <a href=https://fastapi.tiangolo.com/>FastAPI</a> for the backend and <a href=https://svelte.dev/>Svelte</a> for the frontend.</p><p>So let&rsquo;s combine the three!</p><p>TODO: add video demo</p><p>The demo above search over ~1.5M images, taking about 3Gb of space for the index and with latency under 100ms. This is running on single process from my laptop.</p><p>To show off the speed, I prepared a tiny version of all images (&lt;500 bytes) that are returned with the search results and loaded immediately. The full size image is then loaded asynchronously, that can take up to a second as those are random internet images.</p><p>Also human typing speed is not that fast, so to make the search field usable I added a delay before triggering the search, so the latency you see is after the search query is sent.</p><p>TODO: add video demo</p><p>Here is an other demo, with full image loading disabled and a lower typing delay, just to show the speed.</p><p>I&rsquo;m quite pleased with those results, there&rsquo;s still a lot that could be done to speed-up the search further but it&rsquo;s fast enough for a prototype. The index size/accuracy ratio could also be improved a lot, and the CLIP model should be transferable into a smaller inference network. Also I could make a UI that doesn&rsquo;t suck ðŸ˜¬</p><p>Calling this a &ldquo;search engine&rdquo; is a bit of a stretch, there is limited control and no filtering options but the tech stack is very simple (it only took a day to make this demo!). I think simplicity has value in itself. Though with the <a href=https://github.com/elastic/elasticsearch/pull/84734>recent support of ANN+filtering</a> in Lucene/Elasticsearch the comparison would be interesting.</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Paul Willot</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2022-06-15</span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/machine-learning/>machine-learning</a>
<a href=/tags/search/>search</a>
<a href=/tags/web/>web</a></div><nav class=post-nav><a class=next href=/post/denormal-number-at-inference-in-pytorch/><span class="next-text nav-default">Denormal number at inference in PyTorch</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=https://www.linkedin.com/in/paul-willot-903a6b5b/ class="iconfont icon-linkedin" title=linkedin></a><a href=https://github.com/m3at class="iconfont icon-github" title=github></a><a href=/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=copyright-year>&copy;
2020 -
2022
<span>Paul Willot</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.9c0479e84fa822bb8c96fb7d4d5db053030015bee7a6be51aa29f8f6fc1f3c41.js></script></body></html>