<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine-learning on Paul's blog</title><link>/tags/machine-learning/</link><description>Recent content in machine-learning on Paul's blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 18 Sep 2021 11:39:18 +0000</lastBuildDate><atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Denormal number at inference in PyTorch</title><link>/post/denormal-number-at-inference-in-pytorch/</link><pubDate>Sat, 18 Sep 2021 11:39:18 +0000</pubDate><guid>/post/denormal-number-at-inference-in-pytorch/</guid><description>The other day at work I noticed a slowdown in runtime between a model with random weights compared to tuned ones.
It turned out to be due to denormal numbers computation on the cpu being much slower than the normal arithmetic. Denormal numbers are very low magnitude floats, treated differently to keep precision.
For deep learning, those are largely below significance and can be flushed to zero without accuracy loss. To do so for example in PyTorch, either set the appropriate flag: torch.</description></item><item><title>Space filling curves and CNN</title><link>/post/space-filling-curves-and-transformer/</link><pubDate>Sun, 31 Jan 2021 11:59:39 +0000</pubDate><guid>/post/space-filling-curves-and-transformer/</guid><description>üë∑üèª this is a work in progress üîß
Sometimes by exploring incongruous ideas, one can make interesting discoveries. This is know as serendipity in science, with penicillin being a famous example of an unplanned discovery.
What I experimented with this week-end‚Ä¶ is no such case. üòÖ
In the spirit of not holding back negative results though, and for the unlikely possibility that it turns out helpful for someone, I&amp;rsquo;ll share it regardless!</description></item></channel></rss>