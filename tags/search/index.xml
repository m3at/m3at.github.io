<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>search on Paul's blog</title><link>/tags/search/</link><description>Recent content in search on Paul's blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Wed, 15 Jun 2022 04:50:04 +0000</lastBuildDate><atom:link href="/tags/search/index.xml" rel="self" type="application/rss+xml"/><item><title>Real-time semantic search demo</title><link>/post/real-time-semantic-search-demo/</link><pubDate>Wed, 15 Jun 2022 04:50:04 +0000</pubDate><guid>/post/real-time-semantic-search-demo/</guid><description>TLDR: I wanted to make a fast and low complexity &amp;ldquo;search engine&amp;rdquo;, and got good results with neural network embeddings combined with an approximate vector search.
Recent advances in large deep learning models have brought interesting multi-modal capabilities, notably for combined text and images, like Imagen and DALL-E 2 for image synthesis from raw text. CLIP, which is also used in DALL-E for image ranking, allow projecting both text and images into a coherent space.</description></item></channel></rss>