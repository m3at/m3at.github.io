<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Denormal number at inference in PyTorch - Paul's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="Paul Willot"><meta name=description content="The other day at work I noticed a slowdown in runtime between a model with random weights compared to tuned ones.
It turned out to be due to denormal numbers computation on the cpu being much slower than the normal arithmetic. Denormal numbers are very low magnitude floats, treated differently to keep precision.
For deep learning, those are largely below significance and can be flushed to zero without accuracy loss. To do so for example in PyTorch, either set the appropriate flag: torch."><meta name=keywords content="ML,CS"><meta name=generator content="Hugo 0.79.1 with theme even"><link rel=canonical href=/post/denormal-number-at-inference-in-pytorch/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.d70ccb6824a5ce6b30b3758201981307a7fcc24b19562e15fbbdf20fcdf12f61.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Denormal number at inference in PyTorch"><meta property="og:description" content="The other day at work I noticed a slowdown in runtime between a model with random weights compared to tuned ones.
It turned out to be due to denormal numbers computation on the cpu being much slower than the normal arithmetic. Denormal numbers are very low magnitude floats, treated differently to keep precision.
For deep learning, those are largely below significance and can be flushed to zero without accuracy loss. To do so for example in PyTorch, either set the appropriate flag: torch."><meta property="og:type" content="article"><meta property="og:url" content="/post/denormal-number-at-inference-in-pytorch/"><meta property="article:published_time" content="2021-09-18T11:39:18+00:00"><meta property="article:modified_time" content="2021-09-18T11:39:18+00:00"><meta itemprop=name content="Denormal number at inference in PyTorch"><meta itemprop=description content="The other day at work I noticed a slowdown in runtime between a model with random weights compared to tuned ones.
It turned out to be due to denormal numbers computation on the cpu being much slower than the normal arithmetic. Denormal numbers are very low magnitude floats, treated differently to keep precision.
For deep learning, those are largely below significance and can be flushed to zero without accuracy loss. To do so for example in PyTorch, either set the appropriate flag: torch."><meta itemprop=datePublished content="2021-09-18T11:39:18+00:00"><meta itemprop=dateModified content="2021-09-18T11:39:18+00:00"><meta itemprop=wordCount content="153"><meta itemprop=keywords content="programming,machine-learning,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Denormal number at inference in PyTorch"><meta name=twitter:description content="The other day at work I noticed a slowdown in runtime between a model with random weights compared to tuned ones.
It turned out to be due to denormal numbers computation on the cpu being much slower than the normal arithmetic. Denormal numbers are very low magnitude floats, treated differently to keep precision.
For deep learning, those are largely below significance and can be flushed to zero without accuracy loss. To do so for example in PyTorch, either set the appropriate flag: torch."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>paulw.tokyo</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>paulw.tokyo</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Denormal number at inference in PyTorch</h1><div class=post-meta><span class=post-time>2021-09-18</span><div class=post-category><a href=/categories/machine-learning/>machine learning</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents></nav></div></div><div class=post-content><p>The other day at work I noticed a <strong>slowdown in runtime</strong> between a model with random weights compared to tuned ones.</p><p>It turned out to be due to <a href=https://en.m.wikipedia.org/wiki/Subnormal_number>denormal numbers</a> computation on the cpu being much slower than the normal arithmetic. Denormal numbers are very low magnitude floats, treated differently to keep precision.</p><p>For deep learning, those are largely below significance and can be flushed to zero without accuracy loss. To do so for example in PyTorch, either set <a href=https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html title=https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html>the appropriate flag</a>: <code>torch.set_flush_denormal(True)</code>.</p><p>Or manually round down your weights, I choose \(10e-12\) arbitrarily:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>torch</span>

<span class=k>def</span> <span class=nf>flush</span><span class=p>(</span><span class=n>v</span><span class=p>,</span> <span class=n>t</span><span class=o>=</span><span class=mf>1e-12</span><span class=p>):</span>
    <span class=n>v</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>v</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>t</span><span class=p>]</span> <span class=o>=</span> <span class=mf>0.0</span>
    <span class=k>return</span> <span class=n>v</span>

<span class=n>model</span> <span class=o>=</span> <span class=o>...</span>
<span class=n>model</span><span class=o>.</span><span class=n>load_state_dict</span><span class=p>(</span>
    <span class=p>{</span><span class=n>k</span><span class=p>:</span> <span class=n>flush</span><span class=p>(</span><span class=n>v</span><span class=p>)</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>torch</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s2>&#34;./model.pth&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
<span class=p>)</span>

</code></pre></td></tr></table></div></div><p>The speedup will be CPU and model dependent, for me it was quite significant (x2!). Hopefully this tip will save you some inference time too.</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Paul Willot</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2021-09-18</span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/programming/>programming</a>
<a href=/tags/machine-learning/>machine-learning</a></div><nav class=post-nav><a class=next href=/post/placeholder/><span class="next-text nav-default">Placeholder</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=https://www.linkedin.com/in/paul-willot-903a6b5b/ class="iconfont icon-linkedin" title=linkedin></a><a href=https://github.com/m3at class="iconfont icon-github" title=github></a><a href=/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=copyright-year>&copy;
2020 -
2021
<span>Paul Willot</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.9c0479e84fa822bb8c96fb7d4d5db053030015bee7a6be51aa29f8f6fc1f3c41.js></script><script type=text/javascript>window.MathJax={tex:{}};</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>