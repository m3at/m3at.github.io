<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Space filling curves and CNN - Paul's blog</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content="Paul Willot"><meta name=description content="üë∑üèª this is a work in progress üîß
Sometimes by exploring incongruous ideas, one can make interesting discoveries. This is know as serendipity in science, with penicillin being a famous example of an unplanned discovery.
What I experimented with this week-end‚Ä¶ is no such case. üòÖ
In the spirit of not holding back negative results though, and for the unlikely possibility that it turns out helpful for someone, I&amp;rsquo;ll share it regardless!"><meta name=keywords content="ML,CS"><meta name=generator content="Hugo 0.79.1 with theme even"><link rel=canonical href=/post/space-filling-curves-and-transformer/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.d70ccb6824a5ce6b30b3758201981307a7fcc24b19562e15fbbdf20fcdf12f61.css rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin=anonymous><meta property="og:title" content="Space filling curves and CNN"><meta property="og:description" content="üë∑üèª this is a work in progress üîß
Sometimes by exploring incongruous ideas, one can make interesting discoveries. This is know as serendipity in science, with penicillin being a famous example of an unplanned discovery.
What I experimented with this week-end‚Ä¶ is no such case. üòÖ
In the spirit of not holding back negative results though, and for the unlikely possibility that it turns out helpful for someone, I&rsquo;ll share it regardless!"><meta property="og:type" content="article"><meta property="og:url" content="/post/space-filling-curves-and-transformer/"><meta property="article:published_time" content="2021-01-31T11:59:39+00:00"><meta property="article:modified_time" content="2021-01-31T11:59:39+00:00"><meta itemprop=name content="Space filling curves and CNN"><meta itemprop=description content="üë∑üèª this is a work in progress üîß
Sometimes by exploring incongruous ideas, one can make interesting discoveries. This is know as serendipity in science, with penicillin being a famous example of an unplanned discovery.
What I experimented with this week-end‚Ä¶ is no such case. üòÖ
In the spirit of not holding back negative results though, and for the unlikely possibility that it turns out helpful for someone, I&rsquo;ll share it regardless!"><meta itemprop=datePublished content="2021-01-31T11:59:39+00:00"><meta itemprop=dateModified content="2021-01-31T11:59:39+00:00"><meta itemprop=wordCount content="604"><meta itemprop=keywords content="programming,machine learning,exploration,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Space filling curves and CNN"><meta name=twitter:description content="üë∑üèª this is a work in progress üîß
Sometimes by exploring incongruous ideas, one can make interesting discoveries. This is know as serendipity in science, with penicillin being a famous example of an unplanned discovery.
What I experimented with this week-end‚Ä¶ is no such case. üòÖ
In the spirit of not holding back negative results though, and for the unlikely possibility that it turns out helpful for someone, I&rsquo;ll share it regardless!"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>paulw.tokyo</a></div><div class=mobile-navbar-icon><span></span><span></span><span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/tags/><li class=mobile-menu-item>Tags</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>paulw.tokyo</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/tags/>Tags</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>Space filling curves and CNN</h1><div class=post-meta><span class=post-time>2021-01-31</span><div class=post-category><a href=/categories/machine-learning/>machine learning</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><a href=#too-many-dimensions>Too many dimensions</a></li><li><a href=#space-filling-curves-to-the-rescue>Space-filling curves to the rescue</a></li></ul></nav></div></div><div class=post-content><p>üë∑üèª this is a work in progress üîß</p><p>Sometimes by exploring incongruous ideas, one can make interesting discoveries. This is know as <a href=https://en.wikipedia.org/wiki/Serendipity>serendipity</a> in science, with penicillin being a famous example of an unplanned discovery.</p><p>What I experimented with this week-end‚Ä¶ is no such case. üòÖ<br>In the spirit of not holding back negative results though, and for the unlikely possibility that it turns out helpful for someone, I&rsquo;ll share it regardless!</p><h1 id=too-many-dimensions>Too many dimensions</h1><p>In computer vision we mostly deal with spatial data, so to make our life easier we often bake into the models the assumption that nearby elements are related. This is what the fundamental convolution operation give us in deep learning models, some bias towards spatial locality.</p><p>However not everything can be applied spatially, particularly on high dimensional spaces. For examples, building blocks designed for natural language processing often consider only one dimension. Even the <a href=https://arxiv.org/abs/1706.03762>transformer model</a>, which is position independent in it&rsquo;s original formulation, has been the focus of numerous researches attempting to <a href=https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html>bring back some locality</a>, notably for efficiency improvements. Wouldn&rsquo;t it be great to be able to use, as is, techniques developed for \(1D\) space on arbitrarily high \(ND\)?</p><p>The straightforward way to achieve it is to flatten whatever dimensions we have in order. For example, with a \(2D\) image we could take each row and concatenate them into a big list, and we&rsquo;re done!<br>The issue is that we will break spatial locality. Neighboring pixels in a row will stay together, but pixel that were close within a column will suddenly have arbitrarily big distances separating them. Instead, we would like to flatten space while keeping the locality property as best as we can.</p><h1 id=space-filling-curves-to-the-rescue>Space-filling curves to the rescue</h1><p>Luckily, mathematicians have been <a href=https://en.wikipedia.org/wiki/Space-filling_curve title="Space-filling curve">studying this problem</a>! I recommend this <a href="https://www.youtube.com/watch?v=3s7h2MHQtxc" title="Hilbert's Curve: Is infinite math useful?">3Blue1Brown</a> for a great visual introduction, but in short the idea is to fill the entirety of space with a single curve.</p><p><img src=/uploads/four-level_z.svg alt="Four iterations of the Z-order curve" title="Four iterations of the Z-order curve"></p><p>There are a few different functions available, like the Peano and Hilbert curves, or most appropriate for us the <a href=https://en.wikipedia.org/wiki/Z-order_curve>Z-order curve</a> (also known as Morton code). The advantage of the later is that, as illustrated in the figure above, on top of filling space it also preserve locality. Well, to an extent at least.</p><p><img src=/uploads/example_z_order_mapping.webp alt="Example of indexing using Z-order curve" title="Example of indexing using Z-order curve"></p><p>Using the Z-order function, we can create an index and use it for mapping from \(ND\) to \(1D\). Using <a href=https://github.com/arogozhnikov/einops>einops</a> to avoid getting lost in the dimensions, it is easy to create a function doing it for PyTorch.</p><p>(TODO: clean-up and publish the code)</p><p>Now that we have a way to flatten space while keeping some locality, we can design an experiment! To keep things fast I used the small CIFAR-10 dataset, which is fast enough to train on, even with a naive training loop implementation. As a bonus, I already had some code lying around that allowed me to reach 3000 samples/second on my aging 1070Ti (loosely based on <a href>Myrtle.ai</a> posts about training speedup; a story for another time).</p><p>I tried 3 configurations:</p><ul><li>A baseline model with the usual 2D convolutions</li><li>A model using 1D convolutions after flattening the input using the Z-order function</li><li>The same 1D model but with naive flattening, and random flattening</li></ul><p>All models are kept as close as possible, using the same 9-layers architecture and swapping the convolutions/batch-norm/pooling to either 2D or 1D.<br>Because convolving on lower dimensions use smaller kernels it result in less parameters, so I compensate by adjusting the layers' width until the number of trainable parameters match. While this is by no mean sufficient to make it an apple to apple comparison, it will have to do!</p><p>(TODO: finish the write-up :D)</p></div><div class=post-copyright><p class=copyright-item><span class=item-title>Author</span>
<span class=item-content>Paul Willot</span></p><p class=copyright-item><span class=item-title>LastMod</span>
<span class=item-content>2021-01-31</span></p></div><footer class=post-footer><div class=post-tags><a href=/tags/programming/>programming</a>
<a href=/tags/machine-learning/>machine learning</a>
<a href=/tags/exploration/>exploration</a></div><nav class=post-nav><a class=prev href=/post/basic-watermark-removal-in-videos/><i class="iconfont icon-left"></i><span class="prev-text nav-default">Basic watermark removal in videos</span>
<span class="prev-text nav-mobile">Prev</span></a>
<a class=next href=/post/creating-a-static-blog-the-convoluted-way/><span class="next-text nav-default">Creating a static blog, the convoluted way</span>
<span class="next-text nav-mobile">Next</span>
<i class="iconfont icon-right"></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=social-links><a href=https://www.linkedin.com/in/paul-willot-903a6b5b/ class="iconfont icon-linkedin" title=linkedin></a><a href=https://github.com/m3at class="iconfont icon-github" title=github></a><a href=/index.xml type=application/rss+xml class="iconfont icon-rss" title=rss></a></div><div class=copyright><span class=copyright-year>&copy;
2020 -
2021
<span>Paul Willot</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin=anonymous></script><script type=text/javascript src=/js/main.min.9c0479e84fa822bb8c96fb7d4d5db053030015bee7a6be51aa29f8f6fc1f3c41.js></script><script type=text/javascript>window.MathJax={tex:{}};</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>